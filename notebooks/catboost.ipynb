{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catbooster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "is_kaggle = False\n",
    "\n",
    "dataset_name = \"hms-harmful-brain-activity-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up for Google Colab.\n",
    "import sys\n",
    "\n",
    "is_colab = True if \"google.colab\" in sys.modules else False\n",
    "\n",
    "if is_colab:\n",
    "    # Mount Google Drive if it is running on Google Colab.\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Get my kaggle API roken.\n",
    "    !mkdir -p /root/.kaggle\n",
    "    !cp /content/drive/MyDrive/.kaggle/kaggle.json /root/.kaggle/\n",
    "    !chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "    # Prepare for directory.\n",
    "    !pip install kaggle\n",
    "    !apt install unzip\n",
    "    !mkdir input output\n",
    "    \n",
    "    !kaggle competitions download -c {dataset_name}\n",
    "    !unzip -o {dataset_name}.zip -d input/{dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries.\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0, 1\"\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "VER = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"./../data/input/train.csv\" if not is_kaggle else \"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "targets = train_df.columns[-6:]\n",
    "\n",
    "print(\"Train shape: \", train_df.shape)\n",
    "print(\"Targets: \", list(targets))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Non-Overlapping Eeg Id Train Data\n",
    "\n",
    "The competition data description says that test data does not have multiple crops from the same eeg_id. Therefore we will train and validate using only 1 crop per eeg_id. There is a discussion about this [discussion](https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_grouped_by_eeg_id = train_df.groupby(\"eeg_id\")\n",
    "\n",
    "train_df_agg = train_df_grouped_by_eeg_id[[\"spectrogram_id\", \"spectrogram_label_offset_seconds\"]].agg(\n",
    "    {\"spectrogram_id\": \"first\",\n",
    "     \"spectrogram_label_offset_seconds\": \"min\"}\n",
    ")\n",
    "train_df_agg.columns = [\"spec_id\", \"min\"]\n",
    "\n",
    "tmp = train_df_grouped_by_eeg_id[[\"spectrogram_id\", \"spectrogram_label_offset_seconds\"]].agg(\n",
    "    {\"spectrogram_label_offset_seconds\": \"max\"}\n",
    ")\n",
    "train_df_agg['max'] = tmp\n",
    "\n",
    "tmp = train_df_grouped_by_eeg_id[[\"patient_id\"]].agg(\"first\")\n",
    "train_df_agg['patient_id'] = tmp\n",
    "\n",
    "tmp = train_df_grouped_by_eeg_id[targets].agg(\"sum\")\n",
    "for target in targets:\n",
    "    train_df_agg[target] = tmp[target].values\n",
    "\n",
    "tmp = train_df_grouped_by_eeg_id[[\"expert_consensus\"]].agg(\"first\")\n",
    "train_df_agg[\"target\"] = tmp\n",
    "\n",
    "train_df_agg = train_df_agg.reset_index()\n",
    "print(\"Train non-overlapped eeg_id shape: \", train_df_agg.shape)\n",
    "\n",
    "train_df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineer\n",
    "First we need to read in all 11k train spectrogram files. Reading thousands of files takes 11 minutes with Pandas. Instead, we can read 1 file from [Brain-Spectrograms dataset](https://www.kaggle.com/datasets/cdeotte/brain-spectrograms), which contains all the 11k spectrograms in less than 1 minute! To use this dataset, set variable READ_SPEC_FILES = False.\n",
    "\n",
    "Next we need to engineer features for our CatBoost model. In this notebook, we just take the mean (over time) of each of the 400 spectrogram frequencies (using middle 10 minutes). This produces 400 features (per each unique eeg id). We can improve CV and LB score by engineering new features (and/or tuning CatBoost).\n",
    "\n",
    "Also here we create features from means and mins and use 10 minute windows and 20 second windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_SPEC_FILES = False\n",
    "FEATURE_ENGINEER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read all spectrograms.\n",
    "spectrograms_dir_path = \"./../data/input/train_spectrograms\"\n",
    "spectrogram_files = os.listdir(spectrograms_dir_path)\n",
    "\n",
    "print(f\"There are {(len(spectrogram_files))} spectrogram parquets\")\n",
    "\n",
    "if READ_SPEC_FILES:\n",
    "    spectrograms = {}\n",
    "    for index, spectrogram_file in enumerate(spectrogram_files):\n",
    "        if index % 100 == 0:\n",
    "            print(f\"{index}, \", end=\"\")\n",
    "        tmp = pd.read_parquet(f\"{spectrograms_dir_path}{spectrogram_file}\")\n",
    "        name = int(spectrogram_file.split(\".\")[0])\n",
    "        spectrograms[name] = tmp.iloc[:, 1:].values\n",
    "else:\n",
    "    spectrograms_npy_path = \"./../data/input/brain-spectrograms/specs.npy\" if not is_kaggle else \"kaggle/input/brain-spectrograms/specs.npy\"\n",
    "    spectrograms = np.load(spectrograms_npy_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Engineer features.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "spec_cols = pd.read_parquet(os.path.join(spectrograms_dir_path, \"1000086677.parquet\")).columns[1:]\n",
    "features = [f\"{col}_mean_10m\" for col in spec_cols]\n",
    "features += [f\"{col}_min_10m\" for col in spec_cols]\n",
    "features += [f\"{col}_mean_20m\" for col in spec_cols]\n",
    "features += [f\"{col}_min_20m\" for col in spec_cols]\n",
    "\n",
    "print(f\"We are creating {len(features)} features for {len(train_df_agg)} rows...\", end=\"\")\n",
    "\n",
    "if FEATURE_ENGINEER:\n",
    "    data = np.zeros((len(train_df_agg), len(features)))\n",
    "    for k in range(len(train_df_agg)):\n",
    "        if k % 100 == 0:\n",
    "            print(f\"{k}, \", end=\"\")\n",
    "        \n",
    "        row = train_df_agg.iloc[k]\n",
    "        r = int((row['min'] + row['max']) // 4)\n",
    "        \n",
    "        # 10 minute window features (means and mins)\n",
    "        x = np.nanmean(spectrograms[row.spec_id][r:r+300, :], axis=0)\n",
    "        data[k, :400] = x\n",
    "        x = np.nanmin(spectrograms[row.spec_id][r:r+300, :], axis=0)\n",
    "        data[k, 400:800] = x\n",
    "        \n",
    "        # 20 second window features (means and mins)\n",
    "        x = np.nanmean(spectrograms[row.spec_id][r+145:r+155, :], axis=0)\n",
    "        data[k, 800:1200] = x\n",
    "        x = np.nanmin(spectrograms[row.spec_id][r+145:r+155, :], axis=0)\n",
    "        data[k, 1200:1600] = x\n",
    "        \n",
    "    train_df_agg[features] = data\n",
    "else:\n",
    "    train_pqt_path = \"./../data/input/brain-spectrograms/train.pqt\" if not is_kaggle else \"kaggle/input/brain-spectrograms/train.pqt\"\n",
    "    train_df_agg = pd.read_parquet(train_pqt_path)\n",
    "\n",
    "print()\n",
    "print(\"New train shape: \", train_df_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CatBoost\n",
    "We use the default settings for CatBoost which are pretty good. We can tune CatBoost manually to improve CV and LB score. Note that CatBoost will automatically use both Kaggle T4 GPUs (when we add parameter task_type='GPU') for super fast training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat, gc\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "print(\"CatBoost version\", cat.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "all_oof = []\n",
    "all_true = []\n",
    "\n",
    "tars = {\"Seizure\": 0, \"LPD\": 1, \"GPD\": 2, \"LRDA\": 3, \"GRDA\": 4, \"Other\": 5}\n",
    "\n",
    "group_k_fold = GroupKFold(n_splits=5)\n",
    "\n",
    "for index, (train_index, valid_index) in enumerate(group_k_fold.split(train_df_agg, train_df_agg.target, train_df_agg.patient_id)):\n",
    "    print(\"#\" * 25)\n",
    "    print(f\"### Fold {index+1}\")\n",
    "    print(f\"### train size {len(train_index)}, valid size {len(valid_index)}\")\n",
    "    print(\"#\" * 25)\n",
    "    \n",
    "    # model = CatBoostClassifier(task_type=\"GPU\", loss_function=\"MultiClass\")\n",
    "    model = CatBoostClassifier(task_type=\"CPU\", loss_function=\"MultiClass\")\n",
    "    \n",
    "    train_pool = Pool(\n",
    "        data=train_df_agg.loc[train_index, features],\n",
    "        label=train_df_agg.loc[train_index, \"target\"].map(tars),\n",
    "    )\n",
    "    \n",
    "    valid_pool = Pool(\n",
    "        data=train_df_agg.loc[valid_index, features],\n",
    "        label=train_df_agg.loc[valid_index, \"target\"].map(tars),\n",
    "    )\n",
    "    \n",
    "    model.fit(train_pool, verbose=100, eval_set=valid_pool)\n",
    "    \n",
    "    oof = model.predict_proba(valid_pool)\n",
    "    all_oof.append(oof)\n",
    "    all_true.append(train_df_agg.loc[valid_index, targets].values)\n",
    "    \n",
    "    del train_pool, valid_pool, oof\n",
    "    gc.collect()\n",
    "    \n",
    "all_oof = np.concatenate(all_oof)\n",
    "all_true = np.concatenate(all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [CatBoost Starter](https://www.kaggle.com/code/cdeotte/catboost-starter-lb-0-67)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
